{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ghk2bizXTdhC"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKwV731qTdhJ"},"outputs":[],"source":["# If True, test is running on Colab. Otherwise, test if assumed to be offline.\n","TEST_ON_COLAB = False\n","FOLDERNAME = None  # only used if TEST_ON_COLAB is True\n","\n","assert not (FOLDERNAME is None and TEST_ON_COLAB), \"FOLDERNAME has to be set if TEST_ON_COLAB is True\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRko65MRXpgD"},"outputs":[],"source":["import torch\n","\n","# Set the device to be used for training (cuda:0 if there is a GPU available, otherwise cpu)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"2oTocL4GTdhL"},"source":["# Atividade de programação 3 (AP3)\n","## Rede neural convolucional\n","### Valor: 25 pontos\n","\n","Responda as questões indicadas e preencha os campos indicados com o tag `TODO`."]},{"cell_type":"markdown","metadata":{"id":"YGoW7F6tTdhO"},"source":["Para essa atividade, você deverá utilizar *obrigatoriamente* um dataset de classificação de imagens. Pesquise um dataset de imagens de sua preferência (exceto mnist) utilizando o site `https://openml.org/`. O dataset será baixado através de função `fetch_openml` da biblioteca `scikit-learn`.\n","\n","O código a seguir realiza o download de um dataset com a mesma estrutura de pixels/labels ($X$, $y$) vistos em exemplos de aula. Assume-se que a matriz $X$ (matriz de dados) é tal que cada linha armazena uma imagem linearizada, e $y$ é o vetor de labels. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4abnpyuTTdhP"},"outputs":[],"source":["# TODO: Set the dataset ID\n","DATASET_OPENML_ID = None\n","##########################\n","\n","assert DATASET_OPENML_ID is not None, \"DATASET_OPENML_ID is not set\""]},{"cell_type":"markdown","metadata":{"id":"lz_d0LAZTdhR"},"source":["Utilizaremos uma semente para garantir a reprodutibilidade dos resultados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PyvU8uY5TdhR"},"outputs":[],"source":["SEED = 42"]},{"cell_type":"markdown","metadata":{"id":"UEpX197JXpgJ"},"source":["### Base de dados"]},{"cell_type":"markdown","metadata":{"id":"VAvZSSNpTdhS"},"source":["O código a seguir realiza, se necessário, o download da base de dados. Em seguida, as amostras são carregadas em formato matricial."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRy9QqWzuc32"},"outputs":[],"source":["# Create dataset directory\n","import os\n","\n","if TEST_ON_COLAB:\n","    # This mounts your Google Drive to the Colab VM.\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/drive\")\n","    cache_dir = f\"/content/drive/My Drive/{FOLDERNAME}/dataset/{DATASET_OPENML_ID}\"\n","else:\n","    cache_dir = f\"dataset/{DATASET_OPENML_ID}\"\n","\n","os.makedirs(cache_dir, exist_ok=True)\n","\n","# Fetching the dataset\n","from sklearn.datasets import fetch_openml\n","import numpy as np\n","\n","X_file_path = f\"{cache_dir}/X.npy\"\n","y_file_path = f\"{cache_dir}/y.npy\"\n","\n","# Check if the dataset files already exist\n","if not (os.path.exists(X_file_path) and os.path.exists(y_file_path)):\n","    # Fetch the dataset where X is the data and y is the target\n","    X, y = fetch_openml(DATASET_OPENML_ID, as_frame=False, cache=True, return_X_y=True)\n","\n","    # Save the dataset as numpy arrays\n","    np.save(X_file_path, X.astype(np.float32))\n","    np.save(y_file_path, y)\n","    print(f\"{DATASET_OPENML_ID} dataset downloaded and saved successfully to {cache_dir}.\")\n","else:\n","    X = np.load(X_file_path, allow_pickle=True)\n","    y = np.load(y_file_path, allow_pickle=True)\n","    print(f\"{DATASET_OPENML_ID} dataset already exists in {cache_dir}. Skipping download.\")\n","\n","# Cast to 32-bits float number\n","X = X.astype(np.float32)\n","\n","print(X.shape)\n","print(X.min(), X.max(), X.dtype)"]},{"cell_type":"markdown","metadata":{"id":"IcZMz3V8TdhU"},"source":["A seguir, são exibidas algumas amostras do dataset. Ajuste a variável `image_shape` de acordo com as dimensões das amostras da sua base de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLbwaolYB98P"},"outputs":[],"source":["# TODO: Set the image shape. ##################\n","image_shape = None # e.g. (28, 28), (32, 32, 3)\n","###############################################\n","\n","assert image_shape is not None, \"image_shape is not set\"\n","assert len(image_shape) == 2 or len(image_shape) == 3, \"image_shape should be a tuple of 2 or 3 elements\"\n","\n","import matplotlib.pyplot as plt\n","\n","# Visualize some examples from the dataset.\n","# We show a few examples of training images from each class.\n","classes = [int(class_id) for class_id in np.unique(y)]\n","num_classes = len(classes)\n","samples_per_class = 7\n","for cls in classes:\n","    idxs = np.flatnonzero(y == str(cls))\n","    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n","    for i, idx in enumerate(idxs):\n","        plt_idx = i * num_classes + cls + 1\n","        plt.subplot(samples_per_class, num_classes, plt_idx)\n","        plt.imshow(X[idx].reshape(image_shape).astype('uint8'), cmap='gray')\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rJUW3hLyXpgK"},"source":["**1. (2,5 pontos)** Agora, você precisa particionar o conjunto de dados em dois subconjuntos: (i) treino + validação (train+val) e (ii) teste (test).\n","\n","Utilize a função `train_test_split` do módulo `sklearn.model_selection` para dividir o conjunto de dados (`X` e `y`): 90% dos dados para o subconjunto train+val e 10% para o subconjunto test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNxk5d4FXpgL"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Convert labels from string to integer\n","y = y.astype(int)\n","\n","# Set the random seed for reproducibility\n","np.random.seed(SEED)\n","\n","# TODO: Split the dataset into train and test sets\n","X_train_val, X_test, y_train_val, y_test = None\n","##################################################\n","\n","assert X_train_val is not None, \"X_train_val is not set\"\n","assert y_train_val is not None, \"y_train_val is not set\"\n","assert X_test is not None, \"X_test is not set\"\n","assert y_test is not None, \"y_test is not set\"\n","\n","print(f\"Train+val set size: {len(X_train_val)}\")\n","print(f\"Test set size: {len(X_test)}\")"]},{"cell_type":"markdown","metadata":{"id":"k-_S6d8KXpgL"},"source":["**2. (2,5 pontos)** Agora, você precisa separar o conjunto de treino (train) do conjunto de validação (val).\n","\n","Utilize novamente a função `train_test_split` do módulo `sklearn.model_selection` para dividir o conjunto de dados (`X_train_val` e `y_train_val`): 90% dos dados para o subconjunto train e 10% para o subconjunto val."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SmSAUt69XpgL"},"outputs":[],"source":["# TODO: Split further the train+val dataset into train and validation sets\n","X_train, X_val, y_train, y_val = None\n","##########################################################################\n","\n","assert X_val is not None, \"X_val is not set\"\n","assert y_val is not None, \"y_val is not set\"\n","\n","print(f\"Train set size: {len(X_train)}\")\n","print(f\"Val set size: {len(X_val)}\")"]},{"cell_type":"markdown","metadata":{"id":"HcY-O9DOTdhW"},"source":["### Implementação em Pytorch\n","\n","#### Modelo de rede neural convolucional\n","\n","O model adotado nesta atividade tem como base uma rede residual de 18 camadas (ResNet18):\n","\n","```\n","@InProceedings{He_2016_CVPR,\n","author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\n","title = {Deep Residual Learning for Image Recognition},\n","booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n","month = {June},\n","year = {2016}\n","}\n","```\n","\n","A última camada (totalmente conectada, `fc`) deve ser ajustada de acordo com o número de saídas esperadas do modelo (`n_outputs`). Além disso, é possível ajustar se o modelo será treinado do zero ou se será feito o fine-tuning de um modelo pré-treinado na Imagenet. O treinamento pode ser realizado com regularização por dropout se atribuído um valor maior do que zero para o `dropout_rate`. Pode ser aplicado um ajuste fino em todas as camadas ou apenas na última camada (neste caso, `freeze=True`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"so0JVaQwTdhX"},"outputs":[],"source":["from torchvision import models\n","\n","\n","class Resnet18(torch.nn.Module):\n","    def __init__(self, n_output, pretrained=False, dropout_rate=0.5, freeze=False):\n","        super(Resnet18, self).__init__()\n","\n","        # Load ResNet18 model\n","        kwargs = {}\n","        if pretrained:\n","            kwargs[\"weights\"] = models.ResNet18_Weights.IMAGENET1K_V1\n","\n","        self.backbone = models.resnet18(**kwargs)\n","\n","        # Freeze the parameters of the backbone\n","        if freeze:\n","            for param in self.backbone.parameters():\n","                param.requires_grad = False\n","\n","        # Replace the final fully connected layer with dropout + new fully connected layer\n","        num_features = self.backbone.fc.in_features\n","        self.backbone.fc = torch.nn.Sequential(\n","            torch.nn.Dropout(p=dropout_rate),  # Dropout layer\n","            torch.nn.Linear(num_features, n_output),  # New FC layer with n_output classes\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass through the model.\n","\n","        Args:\n","            x: Input data.\n","\n","        Returns:\n","            logits: Raw predictions from the model.\n","        \"\"\"\n","        x = self.backbone(x)  # Forward pass through the backbone to get features\n","        return x\n","\n","    def transforms(self):\n","        return models.ResNet18_Weights.IMAGENET1K_V1.transforms()\n","\n","\n","# Define a the model\n","model = Resnet18(n_output=num_classes, pretrained=True, dropout_rate=0.0, freeze=False)\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"f8aLZUlaXpgM"},"source":["A célula a seguir mostra a implementação da classe Dataset para gerenciamento dos dados.\n","\n","**Importante!**\n","\n","O código assume que as imagens linearizadas seguem formato $N \\times H \\times W \\times C$, onde $N$ é o número de amostras, $H$ é a altura e $W$ é a largura, e $C$ é o número de canais."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8yFEAkxTdhZ"},"outputs":[],"source":["from torchvision.datasets import VisionDataset\n","\n","class Dataset(VisionDataset):\n","    def __init__(self, X, y, image_shape, transforms=None):\n","        super(Dataset, self).__init__(\"\", transforms=transforms)\n","\n","        # Determine height, width, and number of channels\n","        H, W = image_shape[:2] # height, width\n","        C = 1 if len(image_shape) == 2 else image_shape[2] # channels\n","\n","        # Reshape the data to (N, H, W, C)\n","        self.X = X.reshape(-1, H, W, C)\n","\n","        # Transpose the data to (N, C, H, W) format\n","        self.X = np.transpose(self.X, (0, 3, 1, 2))\n","\n","        # Replicate first channel to have 3 channels (enable pretrained models)\n","        if C == 1:\n","            self.X = np.repeat(self.X, 3, axis=1)\n","\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        image = self.X[index]\n","        label = self.y[index]\n","        if self.transform is not None:\n","            image = self.transform(label)\n","        return image, label\n","\n","# Create the datasets\n","datasets = {\n","    \"train\": Dataset(X_train, y_train, image_shape, model.transforms()),\n","    \"val\": Dataset(X_val, y_val, image_shape, model.transforms()),\n","    \"test\": Dataset(X_test, y_test, image_shape, model.transforms())\n","}\n","\n","# Statiscs of the dataset\n","print(f\"Training dataset shape: {datasets['train'].X.shape}\")\n","print(f\"Validation dataset shape: {datasets['val'].X.shape}\")\n","print(f\"Test dataset shape: {datasets['test'].X.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"a8uwNjkmXpgM"},"source":["O carregamento dos dados em batch é implementado de acordo com objetos da classe `DataLoader`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvkV7BiqTdha"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 32\n","\n","assert batch_size is not None, \"batch_size is not set\"\n","\n","dataloaders = {\n","    \"train\": DataLoader(datasets[\"train\"], batch_size=batch_size, shuffle=True),\n","    \"val\": DataLoader(datasets[\"val\"], batch_size=batch_size, shuffle=False),\n","    \"test\": DataLoader(datasets[\"test\"], batch_size=batch_size, shuffle=False)\n","}"]},{"cell_type":"markdown","metadata":{"id":"wJstAPPiXpgM"},"source":["### Treinamento do modelo"]},{"cell_type":"markdown","metadata":{"id":"VAZCjLiyXpgM"},"source":["Antes de realizar o treinamento, configuramos a biblioteca `pytorch` para garantir replicabilidade dos experimentos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afYmoP5DTdha"},"outputs":[],"source":["import torch\n","\n","# Set the random seed for reproducibility\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"A2UsHvT6XpgN"},"source":["A seguir, tem-se a implementação do treinamento do modelo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i53-1bpGTdhb"},"outputs":[],"source":["import time\n","\n","def train(model, dataloaders, learning_rate=0.001, weight_decay=0.0, num_epochs=10, checkpoint=\"model.pt\"):\n","    \"\"\"\n","    Train the model.\n","\n","    Args:\n","        model: The model to train.\n","        dataloaders: The dataloaders.\n","        learning_rate: The learning rate.\n","        weight_decay: The weight decay.\n","        num_epochs: The number of epochs.\n","        checkpoint: The path to the checkpoint.\n","    \"\"\"\n","\n","    # Cross-entropy loss\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    # Optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","    # Initialize the best (validation) accuracy\n","    best_acc = -1\n","    best_epoch = -1\n","\n","    # Training loss and validation accuracy history\n","    train_loss_history = []\n","    val_acc_history = []\n","\n","    # Initialize the time\n","    since = time.time()\n","\n","    # Training loop\n","    for epoch in range(1, num_epochs + 1):\n","        print(f\"Epoch {epoch}/{num_epochs}\")\n","        print(\"-\" * 10)\n","\n","        # Each epoch has a training, validation, and test phase\n","        for phase in [\"train\", \"val\"]:\n","            if phase == \"train\":\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()  # Set model to evaluate mode\n","\n","            # Reset loss and counter of correct predictions\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == \"train\"):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == \"train\":\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data).item()\n","\n","            num_samples = len(datasets[phase])\n","            epoch_loss = running_loss / num_samples\n","            epoch_acc = running_corrects / num_samples\n","\n","            # Update history\n","            if phase == \"train\":\n","                train_loss_history.append(epoch_loss)\n","            if phase == \"val\":\n","                val_acc_history.append(epoch_acc)\n","\n","            # Calculate loss and accuracy\n","            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n","\n","            # Save the (current) best model (based on the validation accuracy)\n","            if phase == \"val\" and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_epoch = epoch\n","                torch.save(model.state_dict(), checkpoint)\n","\n","    time_elapsed = time.time() - since\n","    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n","    print(f\"Best validation accuracy: {best_acc:.4f} at epoch {best_epoch}\")\n","\n","    return train_loss_history, val_acc_history, best_epoch\n","\n","# Train the model for 10 epochs\n","train_loss_history, val_acc_history, best_epoch = train(model, dataloaders, learning_rate=0.001, num_epochs=10)"]},{"cell_type":"markdown","metadata":{"id":"HkL5V5x-XpgN"},"source":["O treinamento produziu as seguintes curvas de aprendizado:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOoOtiulMsB1"},"outputs":[],"source":["# Plot training curve\n","import matplotlib.pyplot as plt\n","\n","fig, ax1 = plt.subplots()\n","\n","# Training loss curve\n","epochs = range(1, len(train_loss_history) + 1)\n","ax1.plot(epochs, train_loss_history, label='Training Loss', color='tab:red')\n","ax1.set_xlabel('Epochs')\n","ax1.set_ylabel('Loss', color='tab:red')\n","ax1.tick_params(axis='y', labelcolor='tab:red')\n","ax1.set_xticks(epochs)\n","\n","# Validation accuracy curve\n","ax2 = ax1.twinx()  # instantiate a second Axes that shares the same x-axis\n","ax2.plot(epochs, val_acc_history, label='Validation Accuracy', color='tab:blue')\n","ax2.set_ylabel('Accuracy', color='tab:blue')\n","ax2.tick_params(axis='y', labelcolor='tab:blue')\n","\n","# Highlight the best epoch\n","plt.vlines(best_epoch, 0, 1, colors='k', linestyles='dashed', label='Best Epoch', color='gray')\n","\n","# Additional information\n","# Legend\n","fig.tight_layout()\n","fig.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3)\n","\n","plt.title(\"Training Curve\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HXKnNp-qTdhd"},"source":["### Teste\n","\n","A partição de teste é utilizada para avaliar o modelo treinado:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwfZt5YbTdhe"},"outputs":[],"source":["def test(checkpoint=\"model.pt\"):\n","    \"\"\"\n","    Test the model.\n","\n","    Args:\n","        checkpoint: The path to the model checkpoint\n","    \"\"\"\n","    params = torch.load(checkpoint, map_location=device)\n","    model.load_state_dict(params)\n","    model.eval()\n","\n","    pred_labels = []\n","    true_labels = []\n","\n","    # Iterate over data\n","    for inputs, labels in dataloaders[\"test\"]:\n","        inputs = inputs.to(device)\n","        outputs = model(inputs)\n","        pred_labels_batch = torch.argmax(outputs, dim=1)\n","        pred_labels.append(pred_labels_batch)\n","        true_labels.append(labels)\n","\n","    pred_labels = torch.cat(pred_labels, dim=0).cpu().numpy()\n","    true_labels = torch.cat(true_labels, dim=0).numpy()\n","\n","    return pred_labels, true_labels\n","\n","# Test the model\n","pred_labels, true_labels = test()"]},{"cell_type":"markdown","metadata":{"id":"NdtGKVksXpgO"},"source":["Os resultados obtidos são exibidos em formato de matriz de confusão e métricas de avaliação (relatório de classificação):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msMM1SBOTdhe"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n","\n","def print_classification_report(true_labels, pred_labels, num_classes):\n","    \"\"\"\n","    Print the classification report.\n","    \"\"\"\n","    # Compute classification report\n","    class_report = classification_report(true_labels, pred_labels, target_names=[str(class_id) for class_id in range(num_classes)])\n","    print(\"\\nClassification Report:\")\n","    print(class_report)\n","\n","\n","def plot_confusion_matrix(true_labels, pred_labels, num_classes):\n","    \"\"\"\n","    Plot the confusion matrix.\n","    \"\"\"\n","    # Compute confusion matrix\n","    cm = confusion_matrix(true_labels, pred_labels, labels=[class_id for class_id in range(num_classes)])\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[class_id for class_id in range(num_classes)])\n","    disp.plot()\n","    # plt.show()\n","\n","\n","print_classification_report(true_labels, pred_labels, num_classes)\n","plot_confusion_matrix(true_labels, pred_labels, num_classes)"]},{"cell_type":"markdown","metadata":{"id":"8PepxUdxXpgP"},"source":["**3. (5 pontos)** Investigue a influência da regularização por dropout no desempenho do modelo. Fixando os demais parâmetros do modelo, varie o valor de dropout_rate entre 0.1 e 0.5 e avalie o desempenho do modelo. Exiba um gráfico da acurácia em função do dropout_rate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFNSpRUuXpgP"},"outputs":[],"source":["dropout_rate_vals = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n","accuracies = []\n","for dropout_rate in dropout_rate_vals:\n","    # TODO: Treino/teste\n","    pass\n","    ####################\n","\n","assert len(accuracies) > 0\n","\n","# Plot training curve\n","import matplotlib.pyplot as plt\n","\n","plt.plot(dropout_rate_vals, accuracies, marker='o')\n","plt.xlabel('Dropout Rate')\n","plt.ylabel('Accuracy')\n","plt.title('Dropout Rate vs. Accuracy')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nU-j7-N_XpgP"},"source":["**4. (5 pontos)** Neste exercício, você investigará o impacto da técnica de aumento de dados na acurácia do modelo. Para isso, foi fornecida a classe `DatasetAugmented` para realização de aumento de dados online. Você deverá refazer os passos de treino e teste do modelo, mas utilizando a classe `DatasetAugmented` para carregar os dados. Compare os resultados obtidos com e sem aumento de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qh5Aa6_-XpgP"},"outputs":[],"source":["%pip install imgaug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBwwzz-EXpgP"},"outputs":[],"source":["import imgaug.augmenters as iaa\n","\n","class DatasetAugmented(Dataset):\n","    def __init__(self, X, y, image_shape, transforms=None):\n","        super(DatasetAugmented, self).__init__(X, y, image_shape, transforms)\n","\n","        # Define the augmentation pipeline\n","        self.augmentation = iaa.Sequential([\n","            iaa.Fliplr(0.5),  # horizontal flips\n","            iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.5))),  # Gaussian blur\n","            iaa.ContrastNormalization((0.75, 1.5)),  # Strengthen or weaken the contrast in each image.\n","            iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5),  # Add gaussian noise.\n","            iaa.Affine(\n","                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},  # scale images to 80-120% of their size\n","                translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},  # translate by -20 to +20 percent (per axis)\n","                rotate=(-25, 25),  # rotate by -25 to +25 degrees\n","                shear=(-8, 8)  # shear by -8 to +8 degrees\n","            )\n","        ])\n","\n","    def __getitem__(self, index):\n","        image, label = super(DatasetAugmented, self).__getitem__(index)\n","        image = self.augmentation(image=image)\n","        return image, label\n","\n","# TODO: Augmented dataset ###\n","datasets_augmented = None\n","dataloaders_augmented = None\n","#############################\n","\n","# TODO: Train the model with augmented data\n","pass\n","###########################################\n","\n","# Test the model ##############\n","pred_labels, true_labels = None\n","###############################\n","\n","assert datasets_augmented is not None, \"datasets_augmented is not set\"\n","assert dataloaders_augmented is not None, \"dataloaders_augmented is not set\"\n","assert pred_labels is not None, \"pred_labels is not set\"\n","assert true_labels is not None, \"true_labels is not set\"\n","\n","# Results\n","print_classification_report(true_labels, pred_labels, num_classes)\n","plot_confusion_matrix(true_labels, pred_labels, num_classes)"]},{"cell_type":"markdown","metadata":{"id":"9mW1TI-CXpgU"},"source":["**5. (10 pontos)** Neste exercício final, você trabalhará pará obter o melhor modelo variando três hiperparâmetros: taxa de aprendizado, taxa de dropout e peso (coeficience) de regularização L2 (`weight_decay`). Você deverá treinar os diferentes modelos e utilizar, no teste, aquele que resultar na maior acurácia no conjunto de **VALIDAÇÃO**.\n","\n","Dica: salve um *checkpoint* para cada configuração."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFwedbhVXpgU"},"outputs":[],"source":["best_checkpoint = None\n","best_val_acc = -1\n","\n","learning_rate_vals = [0.001, 0.0001, 0.00001]\n","dropout_rate_vals = [0.0, 0.5]\n","weight_decay_vals = [0.0, 0.001, 0.0001]\n","\n","for learning_rate in learning_rate_vals:\n","    for weight_decay in weight_decay_vals:\n","        for dropout_rate in dropout_rate_vals:\n","            # TODO: Train and test the models in different setups\n","            pass\n","            ####################################################\n","\n","\n","# Test the best model and report results\n","assert best_checkpoint is not None\n","\n","print(f\"Best checkpoint={best_checkpoint} (val. acc.={best_val_acc:.3f})\")\n","\n","pred_labels, true_labels = test(checkpoint=best_checkpoint)\n","print_classification_report(true_labels, pred_labels, num_classes)\n","plot_confusion_matrix(true_labels, pred_labels, num_classes)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
