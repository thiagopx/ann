{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgD3EZuHZiHb"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "KixLPTj6ZiHf"
      },
      "outputs": [],
      "source": [
        "# If True, test is running on Colab. Otherwise, test if assumed to be offline.\n",
        "TEST_ON_COLAB = False\n",
        "FOLDERNAME = None  # only used if TEST_ON_COLAB is True\n",
        "\n",
        "assert not (FOLDERNAME is None and TEST_ON_COLAB), \"FOLDERNAME has to be set if TEST_ON_COLAB is True\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWY1wx18ZiHg"
      },
      "source": [
        "# Atividade de programação 1 (AP1)\n",
        "## Classificador softmax: 15 pontos\n",
        "\n",
        "Responda as questões indicadas e preencha os campos indicados com o tag `TODO`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parte I: Preparação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nod8R4zZiHi"
      },
      "source": [
        "**1. (0,5 pontos)** Pesquise um dataset de classificação (duas ou mais classes) de sua preferência (exceto mnist) utilizando o site `https://openml.org/`. O download do dataset será realizado por meio da função `fetch_openml` da biblioteca `scikit-learn`. Para essa atividade, o dataset deve ter a mesma estrutura de dados e rótulos ($X$, $y$) vistos em exemplos de aula.\n",
        "\n",
        "Dica: verifique datasets famosos em `https://medium.com/data-science-bootcamp/famous-machine-learning-datasets-you-need-to-know-dd031bf74dd`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF3B8v2cZiHj"
      },
      "outputs": [],
      "source": [
        "# TODO: Set the dataset ID #\n",
        "DATASET_OPENML_ID = None\n",
        "############################\n",
        "\n",
        "assert DATASET_OPENML_ID is not None, \"DATASET_OPENML_ID is not set\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRy9QqWzuc32"
      },
      "outputs": [],
      "source": [
        "# Create dataset directory\n",
        "\n",
        "import os\n",
        "\n",
        "if TEST_ON_COLAB:\n",
        "    # This mounts your Google Drive to the Colab VM.\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "    cache_dir = f\"/content/drive/My Drive/{FOLDERNAME}/dataset/{DATASET_OPENML_ID}\"\n",
        "else:\n",
        "    cache_dir = f\"dataset/{DATASET_OPENML_ID}\"\n",
        "\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "# Fetching the dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "\n",
        "X_file_path = f\"{cache_dir}/X.npy\"\n",
        "y_file_path = f\"{cache_dir}/y.npy\"\n",
        "\n",
        "# Check if the dataset files already exist\n",
        "if not (os.path.exists(X_file_path) and os.path.exists(y_file_path)):\n",
        "    # Fetch the dataset where X is the data and y is the target\n",
        "    X, y = fetch_openml(DATASET_OPENML_ID, as_frame=False, cache=True, return_X_y=True)\n",
        "\n",
        "    # Save the dataset as numpy arrays\n",
        "    np.save(X_file_path, X.astype(np.float32))\n",
        "    np.save(y_file_path, y)\n",
        "    print(f\"{DATASET_OPENML_ID} dataset downloaded and saved successfully to {cache_dir}.\")\n",
        "else:\n",
        "    X = np.load(X_file_path, allow_pickle=True)\n",
        "    y = np.load(y_file_path, allow_pickle=True)\n",
        "    print(f\"{DATASET_OPENML_ID} dataset already exists in {cache_dir}. Skipping download.\")\n",
        "\n",
        "print(X.shape)\n",
        "print(X.min(), X.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix5qMhjIZiHk"
      },
      "source": [
        "**2. (1 ponto)** Calcule o número de classes do dataset com bases no vetor de rótulos $y$. Em seguida, calcule o número de features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "l7lLtF8pZiHl"
      },
      "outputs": [],
      "source": [
        "# TODO: Calculate the number of classes #\n",
        "num_classes = None\n",
        "#########################################\n",
        "\n",
        "# TODO: Calculate the number of features #\n",
        "num_features = None\n",
        "##########################################\n",
        "\n",
        "assert num_classes is not None, \"num_classes is not set\"\n",
        "assert num_features is not None, \"num_features is not set\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNsMowY9ZiHl"
      },
      "source": [
        "Visualizando algumas amostras do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLbwaolYB98P"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize some examples from the dataset.\n",
        "# We show a few examples of training images from each class.\n",
        "classes = [int(class_id) for class_id in np.unique(y)]\n",
        "samples_per_class = 7\n",
        "for cls in classes:\n",
        "    idxs = np.flatnonzero(y == str(cls))\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + cls + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow((255 * X[idx, :-1]).reshape((28, -1)).astype('uint8'), cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuon5FLVZiHo"
      },
      "source": [
        "### Parte II: Implementação do classificador softmax\n",
        "\n",
        "Nesta parte da atividade, abordaremos o processo de treinamento. Para isso, serão defindas algumas funções auxiliares:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "PqTUewmzZiHo"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(Y_pred, class_idxs):\n",
        "    \"\"\"Compute the cross-entropy loss for a batch of exemplars.\n",
        "\n",
        "    Args:\n",
        "        Y_pred: the predicted class probabilities.\n",
        "        class_idxs: the indices of the true classes.\n",
        "    \"\"\"\n",
        "    losses = -np.log(Y_pred[range(len(class_idxs)), class_idxs])\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def cross_entropy_grad(Y_pred, class_idxs):\n",
        "    \"\"\"Compute the gradient of the cross-entropy loss wrt the predicted class probabilities (batch version).\n",
        "\n",
        "    Args:\n",
        "        Y_pred: the predicted class probabilities.\n",
        "        class_idxs: the indices of the true classes.\n",
        "    \"\"\"\n",
        "    grad = -1 / Y_pred[range(len(Y_pred)), class_idxs]  # n,\n",
        "    grad_output = np.zeros_like(Y_pred)  # (n, k)\n",
        "    grad_output[range(len(grad_output)), class_idxs] = grad  # (n, k)\n",
        "    return grad_output  # (n, k)\n",
        "\n",
        "\n",
        "def softmax_grad(grad_output, Y_pred):\n",
        "    \"\"\"Computer the gradient of the loss wrt the logits of the softmax function (batch version).\n",
        "\n",
        "    Args:\n",
        "        grad_output: the gradient of the loss wrt the output of the softmax.\n",
        "        Y_pred: the output of the softmax function.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(grad_output)  # (n, k)\n",
        "    for i in range(len(grad_output)):\n",
        "        y_pred = Y_pred[i].reshape(-1, 1)  # (k, 1)\n",
        "        J = np.dot(-y_pred, y_pred.T)  # (k, 1) * (1, k) => (k, k)\n",
        "        J[np.diag_indices_from(J)] = y_pred.flatten() * (1 - y_pred.flatten())  # (k, k)\n",
        "        grad[i] = np.dot(grad_output[i], J)  # (k,) @ (k, k) => (k,)\n",
        "    return grad\n",
        "\n",
        "\n",
        "def linear_gradW(grad_output, X):\n",
        "    \"\"\"Compute the gradient of the loss wrt the output of a linear layer (batch version).\n",
        "\n",
        "    Args:\n",
        "        grad_output: the gradient of the loss wrt the output of the linear layer.\n",
        "        X: the input of the linear layer.\n",
        "\n",
        "    Returns:\n",
        "        the gradient of the loss wrt the weights of the linear layer.\n",
        "    \"\"\"\n",
        "    grad = np.dot(X.T, grad_output)  # (d, n) @ (n, h) => (d, h)\n",
        "\n",
        "    # average over the batch\n",
        "    grad /= X.shape[0]\n",
        "    return grad\n",
        "\n",
        "def softmax(z):\n",
        "    \"\"\"Softmax function (batch version).\n",
        "\n",
        "    Args:\n",
        "        z: the input of the softmax function.\n",
        "\n",
        "    Returns:\n",
        "        the output of the softmax function (probabilities).\n",
        "    \"\"\"\n",
        "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Treinamento do modelo: versão I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLIApHGLZiHn"
      },
      "source": [
        "**3. (1,5 ponto)** Nesta primeira versão, os dados serão particionados em treino e teste apenas. Utilize a função `train_test_split` do módulo `sklearn.model_selection` para dividir o conjunto de dados (`X` e `y`), sendo 90% do total reservado para o treinamento e 10% para teste.\n",
        "\n",
        "Encontre o maior valor nos dados de treinamento e divida todos os valores (treinamento e teste) pelo maior valor encontrado. Em seguida, adicione uma coluna de 1's no vetor de atributos de treinamento e teste. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NJkaDSBCo3w"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Convert labels from string to integer\n",
        "y = y.astype(int)\n",
        "\n",
        "# TODO: Split the dataset into train, validation, and test sets #\n",
        "X_train, X_test, y_train, y_test = None, None, None, None\n",
        "#################################################################\n",
        "\n",
        "assert X_train is not None, \"X_train is not set\"\n",
        "assert y_train is not None, \"y_train is not set\"\n",
        "assert X_test is not None, \"X_test is not set\"\n",
        "assert y_test is not None, \"y_test is not set\"\n",
        "\n",
        "# TODO: Normalize the data ###\n",
        "max_value = None\n",
        "X_train = None\n",
        "X_test = None\n",
        "##############################\n",
        "\n",
        "assert max_value is not None, \"max_value is not set\"\n",
        "assert X_train.min() == 0.0 and X_train.max() == 1.0, \"X_train is not normalized\"\n",
        "assert X_test.min() == 0.0 and X_test.max() == 1.0, \"X_test is not normalized\"\n",
        "\n",
        "# TODO: Add bias to the data #\n",
        "X_train = None\n",
        "X_test = None\n",
        "##############################\n",
        "\n",
        "assert X_train.shape[1] == num_features + 1, \"X_train does not have the correct shape\"\n",
        "assert X_test.shape[1] == num_features + 1, \"X_test does not have the correct shape\"\n",
        "\n",
        "print(f\"Train set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. (3 pontos)** Nesta questão, você deve completar os espaços indicados de modo a implementar o processo de treinamento de um classificador softmax para $k$ classes:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = \\text{softmax}(\\mathbf{W}^T \\mathbf{x}),\n",
        "$$\n",
        "\n",
        "onde $\\mathbf{W} \\in \\mathbb{R}^{(d+1) \\times k}$ é a matriz de pesos do classificador, $\\mathbf{x} \\in \\mathbb{R}^{(d+1)}$ é o vetor de entrada e $\\hat{\\mathbf{y}} \\in \\mathbb{R}^{k}$ é a predição do classificador (assumimos que $\\mathbf{x}$ e $\\hat{\\mathbf{y}}$ são vetores coluna).\n",
        "\n",
        "Para *mini-batch gradient descent*, as predições são calculadas como\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{Y}} = \\text{softmax}(\\mathbf{X}\\mathbf{W}),\n",
        "$$\n",
        "\n",
        "onde $\\mathbf{X} \\in \\mathbb{R}^{n \\times (d+1)}$ é a matriz de dados e $\\hat{\\mathbf{Y}} \\in \\mathbb{R}^{n \\times k}$ é a matriz de predições."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q4_GbKFZiHp"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# TODO: Set learning rate #\n",
        "learning_rate = None\n",
        "###########################\n",
        "\n",
        "assert learning_rate is not None, \"learning_rate is not set\"\n",
        "\n",
        "# Batch size\n",
        "batch_size = 256 # adjust as needed\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# Initialize weights\n",
        "W = np.random.normal(size=(X_train.shape[1], num_classes)) * 0.001\n",
        "\n",
        "# Initialize iteration\n",
        "iteration = 0\n",
        "loss_history = []\n",
        "\n",
        "# Training loop\n",
        "pbar = tqdm(total=num_epochs, desc=\"\")\n",
        "for epoch in range(num_epochs):\n",
        "    # Loop over batches\n",
        "    for k in range(0, X_train.shape[0], batch_size):\n",
        "        X_batch = X_train[k:k+batch_size]\n",
        "        y_batch = y_train[k:k+batch_size]\n",
        "\n",
        "        # TODO: Forward pass (compute class probabilities) #\n",
        "        Y_pred = None\n",
        "        ####################################################\n",
        "\n",
        "        assert Y_pred is not None, \"Y_pred is not set\"\n",
        "\n",
        "        # TODO: Compute loss #\n",
        "        loss = None\n",
        "        ######################\n",
        "\n",
        "        assert loss is not None, \"loss is not set\"\n",
        "\n",
        "        # Save the loss\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # TODO: Backward pass (compute gradients) #\n",
        "        grad_pred = None\n",
        "        grad_z = None\n",
        "        grad_W = None\n",
        "        ###########################################\n",
        "\n",
        "        assert grad_pred is not None, \"grad_pred is not set\"\n",
        "        assert grad_z is not None, \"grad_z is not set\"\n",
        "        assert grad_W is not None, \"grad_W is not set\"\n",
        "\n",
        "        # TODO: Update weights ################################\n",
        "        W = None\n",
        "        #######################################################\n",
        "\n",
        "        assert W is not None, \"W is not set\"\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "        if iteration % 100 == 0:\n",
        "            pbar.set_description(f\"Epoch loss: {loss:.3f}\")\n",
        "\n",
        "    # Update progress bar\n",
        "    pbar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confira a curva de aprendizado do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def smooth_loss_history(loss_history, window_size=10):\n",
        "    \"\"\" Smooth the loss history for better visualization.\n",
        "    \n",
        "    Args:\n",
        "        loss_history (numpy.ndarray): The loss history.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The smoothed loss history.\n",
        "    \"\"\"\n",
        "    kernel = np.ones(window_size) / window_size\n",
        "    smoothed_loss_history = np.convolve(loss_history, kernel, mode=\"same\")\n",
        "    return smoothed_loss_history\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "color = \"tab:red\"\n",
        "ax1.set_xlabel(\"Iteration\")\n",
        "ax1.set_ylabel(\"Loss (train)\", color=color)\n",
        "ax1.plot(smooth_loss_history(loss_history), color=color)\n",
        "ax1.tick_params(axis=\"y\", labelcolor=color)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Resultados (inferência)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**5. (1 ponto)** Realize as inferências finais no conjunto de teste preenchendo a célula abaixo. Visualize o resultado do teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# TODO: Predict classes on test set #\n",
        "y_pred =  None\n",
        "#####################################\n",
        "\n",
        "assert y_pred is not None, \"y_pred is not set\"\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[class_id for class_id in range(10)])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[class_id for class_id in range(10)])\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# Compute classification report\n",
        "class_report = classification_report(y_test, y_pred, target_names=[str(class_id) for class_id in range(10)])\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**6. (2 pontos)** Nesta segunda versão, os dados serão particionados em treino, validação e teste. Utilize a função `train_test_split` do módulo `sklearn.model_selection` para subdividir o conjunto de treinamento previamente determinado em treinamento (90%) e validação (10%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Split the train dataset into train and validation #\n",
        "X_train, X_val, y_train, y_val = None, None, None, None\n",
        "###########################################################\n",
        "\n",
        "assert X_train is not None, \"X_train is not set\"\n",
        "assert y_train is not None, \"y_train is not set\"\n",
        "assert X_val is not None, \"X_val is not set\"\n",
        "assert y_val is not None, \"y_val is not set\"\n",
        "\n",
        "print(f\"Train set size: {len(X_train)}\")\n",
        "print(f\"Val set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**7. (6 pontos)** Agora, você deve implementar o processo de treinamento utilizando a partição validação para determinar a o melhor modelo considerando cada época de treinamento (`best-epoch model`). A mesma configuração de treinamento da versão I deve ser utilizada, mas agora com a partição de validação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(SEED)\n",
        "\n",
        "# Initialize weights\n",
        "W = np.random.normal(size=(X_train.shape[1], num_classes)) * 0.001\n",
        "\n",
        "# Initialize best accuracy, weights, and epoch\n",
        "best_accuracy = -1\n",
        "best_W = None\n",
        "best_epoch = 0\n",
        "accuracy_val = -1  # initially undefined\n",
        "\n",
        "# Initialize iteration\n",
        "iteration = 0\n",
        "\n",
        "# Initialize history\n",
        "loss_history = []\n",
        "accuracy_history = []\n",
        "\n",
        "# Training loop\n",
        "pbar = tqdm(total=num_epochs, desc=\"\")\n",
        "for epoch in range(num_epochs):\n",
        "    # Loop over batches\n",
        "    for k in range(0, X_train.shape[0], batch_size):\n",
        "        X_batch = X_train[k:k+batch_size]\n",
        "        y_batch = y_train[k:k+batch_size]\n",
        "\n",
        "        # TODO: Forward pass (compute class probabilities) ###\n",
        "        Y_pred = None\n",
        "        ######################################################\n",
        "\n",
        "        assert Y_pred is not None, \"Y_pred is not set\"\n",
        "\n",
        "        # TODO: Compute loss #####################\n",
        "        loss = None\n",
        "        ##########################################\n",
        "\n",
        "        assert loss is not None, \"loss is not set\"\n",
        "\n",
        "        # Save the loss\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # TODO: Backward pass (compute gradients) #############\n",
        "        grad_pred = None\n",
        "        grad_z = None\n",
        "        grad_W = None\n",
        "        #######################################################\n",
        "\n",
        "        assert grad_pred is not None, \"grad_pred is not set\"\n",
        "        assert grad_z is not None, \"grad_z is not set\"\n",
        "        assert grad_W is not None, \"grad_W is not set\"\n",
        "\n",
        "        # TODO: Update weights ################################\n",
        "        W = None\n",
        "        #######################################################\n",
        "\n",
        "        assert W is not None, \"W is not set\"\n",
        "\n",
        "        iteration += 1\n",
        "        if iteration % 100 == 0:\n",
        "            pbar.set_description(\n",
        "                f\"Epoch loss: {loss:.3f} Accuracy (val): {accuracy_val:.3f} (best: {best_accuracy:.3f})\"\n",
        "            )\n",
        "\n",
        "    # TODO: Predict classes on validation set #\n",
        "    Y_pred = None\n",
        "    pred_labels = None\n",
        "    ###########################################\n",
        "\n",
        "    # TODO: Compute accuracy on validation set #\n",
        "    accuracy_val =  None\n",
        "    ############################################\n",
        "\n",
        "    assert Y_pred is not None, \"Y_pred is not set\"\n",
        "    assert pred_labels is not None, \"pred_labels is not set\"\n",
        "    assert accuracy_val is not None, \"accuracy_val is not set\"\n",
        "\n",
        "    # Save the accuracy\n",
        "    accuracy_history.append(accuracy_val)\n",
        "\n",
        "    if accuracy_val > best_accuracy:\n",
        "        # TODO: Update best accuracy and best weights #\n",
        "        best_accuracy = None\n",
        "        best_W = None\n",
        "        best_epoch = None\n",
        "        ###############################################\n",
        "\n",
        "    # Update progress bar\n",
        "    pbar.update(1)\n",
        "\n",
        "# Reset weights to the best found\n",
        "W = best_W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confira a curva de aprendizado do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Loss\n",
        "ax1.set_xlabel(\"Iteration\")\n",
        "ax1.set_ylabel(\"Loss (train)\", color=\"tab:red\")\n",
        "handle1 = ax1.plot(smooth_loss_history(loss_history), color=\"tab:red\", label=\"Loss (train)\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
        "\n",
        "# Accuracy\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Accuracy (val)\", color=\"tab:blue\")\n",
        "iterations_by_epoch = len(loss_history) // num_epochs\n",
        "handle2 = ax2.plot(np.arange(0, len(loss_history), iterations_by_epoch), accuracy_history, color=\"tab:blue\", label=\"Accuracy (val)\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
        "ax2.vlines(\n",
        "    best_epoch * iterations_by_epoch, min(accuracy_history), max(accuracy_history), colors=\"gray\", linestyles=\"dashed\"\n",
        ")\n",
        "fig.tight_layout()\n",
        "\n",
        "# Legend and title\n",
        "handles = [handle1[0], handle2[0]]\n",
        "plt.legend(handles, [\"Loss (train)\", \"Accuracy (val)\"], loc=\"upper center\")\n",
        "plt.title(\"Loss and Accuracy vs. Iterations\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Resultados (inferência)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realize as inferências no conjunto de teste de acordo com o teste da primeira versão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fla62oXbZiHq"
      },
      "outputs": [],
      "source": [
        "# TODO: Predict classes on test set ###################\n",
        "y_pred = None\n",
        "#######################################################\n",
        "\n",
        "assert y_pred is not None, \"y_pred is not set\"\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[class_id for class_id in range(10)])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[class_id for class_id in range(10)])\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# Compute classification report\n",
        "class_report = classification_report(y_test, y_pred, target_names=[str(class_id) for class_id in range(10)])\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
