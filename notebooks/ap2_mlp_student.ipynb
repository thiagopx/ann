{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wgD3EZuHZiHb"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KixLPTj6ZiHf"
      },
      "outputs": [],
      "source": [
        "# If True, test is running on Colab. Otherwise, test if assumed to be offline.\n",
        "TEST_ON_COLAB = False\n",
        "FOLDERNAME = None  # only used if TEST_ON_COLAB is True\n",
        "\n",
        "assert not (FOLDERNAME is None and TEST_ON_COLAB), \"FOLDERNAME has to be set if TEST_ON_COLAB is True\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWY1wx18ZiHg"
      },
      "source": [
        "# Atividade de programação 2 (AP2)\n",
        "## Multi-layer perceptron: 20 pontos\n",
        "\n",
        "Responda as questões indicadas e preencha os campos indicados com o tag `TODO`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parte I: Preparação dos dados\n",
        "\n",
        "Reutilize o código da atividade anterior para carregar os dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CF3B8v2cZiHj"
      },
      "outputs": [],
      "source": [
        "# TODO: Set the dataset ID ####\n",
        "DATASET_OPENML_ID = None\n",
        "###############################\n",
        "\n",
        "# TODO: Is your dataset an image dataset? #\n",
        "IS_IMAGE_DATASET = None\n",
        "###########################################\n",
        "\n",
        "assert DATASET_OPENML_ID is not None, \"DATASET_OPENML_ID is not set\"\n",
        "assert IS_IMAGE_DATASET is not None, \"IS_IMAGE_DATASET is not set\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SRy9QqWzuc32"
      },
      "outputs": [],
      "source": [
        "# Create dataset directory\n",
        "\n",
        "import os\n",
        "\n",
        "if TEST_ON_COLAB:\n",
        "    # This mounts your Google Drive to the Colab VM.\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "    cache_dir = f\"/content/drive/My Drive/{FOLDERNAME}/dataset/{DATASET_OPENML_ID}\"\n",
        "else:\n",
        "    cache_dir = f\"dataset/{DATASET_OPENML_ID}\"\n",
        "\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "# Fetching the dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "\n",
        "X_file_path = f\"{cache_dir}/X.npy\"\n",
        "y_file_path = f\"{cache_dir}/y.npy\"\n",
        "\n",
        "# Check if the dataset files already exist\n",
        "if not (os.path.exists(X_file_path) and os.path.exists(y_file_path)):\n",
        "    # Fetch the dataset where X is the data and y is the target\n",
        "    X, y = fetch_openml(DATASET_OPENML_ID, as_frame=False, cache=True, return_X_y=True)\n",
        "\n",
        "    # Save the dataset as numpy arrays\n",
        "    np.save(X_file_path, X.astype(np.float32))\n",
        "    np.save(y_file_path, y)\n",
        "    print(f\"{DATASET_OPENML_ID} dataset downloaded and saved successfully to {cache_dir}.\")\n",
        "else:\n",
        "    X = np.load(X_file_path, allow_pickle=True)\n",
        "    y = np.load(y_file_path, allow_pickle=True)\n",
        "    print(f\"{DATASET_OPENML_ID} dataset already exists in {cache_dir}. Skipping download.\")\n",
        "\n",
        "\n",
        "# Preprocess the dataset\n",
        "if IS_IMAGE_DATASET:\n",
        "    X = X / 255.0\n",
        "\n",
        "print(X.shape)\n",
        "print(X.min(), X.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix5qMhjIZiHk"
      },
      "source": [
        "A seguir, são calculados: o número de classes do dataset e o número de features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "l7lLtF8pZiHl"
      },
      "outputs": [],
      "source": [
        "num_classes = len(np.unique(y)) # Number of classes in the dataset\n",
        "num_features = X.shape[1] # Number of features in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNsMowY9ZiHl"
      },
      "source": [
        "Visualizando algumas amostras do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YLbwaolYB98P"
      },
      "outputs": [],
      "source": [
        "if IS_IMAGE_DATASET:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Visualize some examples from the dataset.\n",
        "    # We show a few examples of training images from each class.\n",
        "    classes = [int(class_id) for class_id in np.unique(y)]\n",
        "    samples_per_class = 7\n",
        "    for cls in classes:\n",
        "        idxs = np.flatnonzero(y == str(cls))\n",
        "        idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "        for i, idx in enumerate(idxs):\n",
        "            plt_idx = i * num_classes + cls + 1\n",
        "            plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "            img = X[idx].reshape((28, -1))\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "            if i == 0:\n",
        "                plt.title(cls)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. (2 pontos)** Utilize a função `train_test_split` do módulo `sklearn.model_selection` para dividir o conjunto de dados (`X` e `y`) em treino (80%), validação (10%) e teste (10%). O conjunto de validação será utilizado exclusivemente na última questão (ponto extra)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Convert labels from string to integer\n",
        "y = y.astype(int)\n",
        "\n",
        "# TODO: Split the dataset into train, test, and validation sets #\n",
        "X_train, X_test, y_train, y_test = None, None, None, None\n",
        "X_val, X_test, y_val, y_test = None, None, None, None\n",
        "#################################################################\n",
        "\n",
        "assert X_train is not None, \"X_train is not set\"\n",
        "assert y_train is not None, \"y_train is not set\"\n",
        "assert X_val is not None, \"X_val is not set\"\n",
        "assert y_val is not None, \"y_val is not set\"\n",
        "assert X_test is not None, \"X_test is not set\"\n",
        "assert y_test is not None, \"y_test is not set\"\n",
        "\n",
        "print(f\"Train set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. (5 pontos)** Padronize os dados de entrada para que cada feature tenha média 0 e desvio padrão 1. Dica: utilize o `StandardScaler` do `sklearn`. Verifique a [documentação](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html) da biblioteca para entender como utilizar essa funcionalidade. Ao final, adicione a coluna de bias ao conjunto de dados de treino, validação e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# TODO: Fit the scaler on the training data and transform the training, validation, and test data #\n",
        "X_train_scaled = None\n",
        "X_val_scaler = None\n",
        "X_test_scaler = None\n",
        "##################################################################################################\n",
        "\n",
        "assert X_train_scaled is not None, \"X_train_scaled is not set\"\n",
        "assert X_val_scaler is not None, \"X_val_scaler is not set\"\n",
        "assert X_test_scaler is not None, \"X_test_scaler is not set\"\n",
        "\n",
        "# Ensure that the standard deviation of the features is close to 1\n",
        "stds =  X_train_scaled.std(axis=0)   \n",
        "idx_pos = stds == 0 # where the standard deviation is zero\n",
        "idxs_one = abs(1 - stds) < 1.e-02 # where the standard deviation is close to one\n",
        "assert np.logical_or(idx_pos, idxs_one).all()\n",
        "\n",
        "# TODO: Add bias to the data #\n",
        "X_train_bias = None\n",
        "X_val_bias = None\n",
        "X_test_bias = None\n",
        "##############################\n",
        "\n",
        "assert X_train_bias is not None, \"X_train_bias is not set\"\n",
        "assert X_val_bias is not None, \"X_val_bias is not set\"\n",
        "assert X_test_bias is not None, \"X_test_bias is not set\"\n",
        "\n",
        "# Ensure that the bias term is added correctly\n",
        "assert X_train_bias.shape[1] == num_features + 1, \"X_train does not have the correct shape\"\n",
        "assert X_test_bias.shape[1] == num_features + 1, \"X_test does not have the correct shape\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuon5FLVZiHo"
      },
      "source": [
        "### Parte II: Treinamento e Teste\n",
        "\n",
        "Nesta parte da atividade, abordaremos o treinamento e teste de um modelo MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. (5 pontos)** Defina um classificador [MLP](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) com os seguintes hiperparâmetros:\n",
        "\n",
        "- Camada oculta com 64 neurônios e função de ativação ReLU.\n",
        "- Número máximo de épocas de 100.\n",
        "- Batch size de 32.\n",
        "- Taxa de aprendizado de 0.001.\n",
        "- Early stopping com `n_iter_to_change=10`.\n",
        "\n",
        "Os demais hiperparâmetros devem ser mantidos com os valores padrão. Para acompanhar o treinamento, utilize o parâmetro `verbose=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0NJkaDSBCo3w"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# TODO: Create a neural network model #\n",
        "model = None\n",
        "#######################################\n",
        "\n",
        "assert model is not None, \"model is not set\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. (1 ponto)** A seguir, treine o modelo utilizando o conjunto de treino. Neste caso, o conjunto de validação é definido implicitamente pelo parâmetro `validation_fraction=0.1`. O conjunto `X_val` e `y_val` não são necessários, por enquanto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train the model on the training data #\n",
        "pass\n",
        "##############################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Resultados (inferência)\n",
        "\n",
        "**5. (1 ponto)** Realize a inferência do modelo treinado no conjunto de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Evaluate the model on the test data #\n",
        "y_pred = None\n",
        "#############################################\n",
        "\n",
        "assert y_pred is not None, \"y_pred is not set\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# Compute classification report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parte III: Grid search\n",
        "\n",
        "Nesta parte da atividade, abordaremos a busca exaustiva de parâmetros (grid search)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**6. (5 pontos)** Defina o espaço de hiperparâmetros para implementação do grid search de acordo com as seguintes especificações:\n",
        "\n",
        "- Camada oculta com 64, 128 e 256 neurônios.\n",
        "- Função de ativação ReLU e tanh.\n",
        "- Otimizador Adam e SGD.\n",
        "- Taxa de aprendizado de 0.01, 0.001 e 0.0001.\n",
        "- Batch size de 32.\n",
        "- Número máximo de épocas de 100.\n",
        "- Early stopping com `n_iter_to_change=10`.\n",
        "\n",
        "Os demais hiperparâmetros devem ser mantidos com os valores padrão. Para acompanhar o treinamento, utilize o parâmetro `verbose=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q4_GbKFZiHp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# TODO: Set the parameters by cross-validation\n",
        "parameters = None\n",
        "##############################################\n",
        "\n",
        "assert parameters is not None, \"parameters is not set\"\n",
        "\n",
        "model = MLPClassifier(random_state=SEED, verbose=True)\n",
        "gs = GridSearchCV(model, parameters, cv=5)\n",
        "gs.fit(X_train_scaled, y_train)\n",
        "print(gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Resultados (inferência)\n",
        "\n",
        "**7. (1 ponto)** Realize a inferência do modelo treinado no conjunto de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Evaluate the model on the test data #\n",
        "y_pred = None\n",
        "#############################################\n",
        "\n",
        "assert y_pred is not None, \"y_pred is not set\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# Compute classification report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**8. (2 pontos extra)** Implemente o grid search do modelo MLP utilizando como conjunto de validação `X_val`. Dica: pesquise por [PredefinedSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Grid search #\n",
        "pass\n",
        "#####################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
